{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten\n",
    "from keras.callbacks import TensorBoard\n",
    "from datetime import time\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  type                                             review label  \\\n",
       "0           0  test  Once again Mr. Costner has dragged out a movie...   neg   \n",
       "1           1  test  This is an example of why the majority of acti...   neg   \n",
       "2           2  test  First of all I hate those moronic rappers, who...   neg   \n",
       "3           3  test  Not even the Beatles could write songs everyon...   neg   \n",
       "4           4  test  Brass pictures (movies is not a fitting word f...   neg   \n",
       "\n",
       "          file  \n",
       "0      0_2.txt  \n",
       "1  10000_4.txt  \n",
       "2  10001_1.txt  \n",
       "3  10002_3.txt  \n",
       "4  10003_3.txt  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('imdb_master.csv',encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['review'].values\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing data\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the vocabulary of data\n",
    "sentences = tokenizer.texts_to_matrix(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Dense(300,input_dim=sentences.shape[1], activation='relu'))\n",
    "model.add(layers.Dense(20, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/\", histogram_freq=1, write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "15000/15000 [==============================] - 1s 93us/step - loss: 0.5583 - acc: 0.8035 - val_loss: 0.2975 - val_acc: 0.8774\n",
      "Epoch 2/5\n",
      "15000/15000 [==============================] - 1s 64us/step - loss: 0.2545 - acc: 0.9035 - val_loss: 0.2831 - val_acc: 0.8868\n",
      "Epoch 3/5\n",
      "15000/15000 [==============================] - 1s 64us/step - loss: 0.2252 - acc: 0.9111 - val_loss: 0.2865 - val_acc: 0.8834\n",
      "Epoch 4/5\n",
      "15000/15000 [==============================] - 1s 64us/step - loss: 0.2039 - acc: 0.9213 - val_loss: 0.3119 - val_acc: 0.8730\n",
      "Epoch 5/5\n",
      "15000/15000 [==============================] - 1s 64us/step - loss: 0.1876 - acc: 0.9261 - val_loss: 0.3088 - val_acc: 0.8794\n",
      "5000/5000 [==============================] - 0s 38us/step\n",
      "Evaluation result on Test Data : Loss = 0.3088232529461384, accuracy = 0.8794\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, epochs=5, verbose=True, validation_data=(X_test,y_test), batch_size=256,callbacks=[tensorboard])\n",
    "[test_loss, test_acc] = model.evaluate(X_test, y_test)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [0.2974769147872925,\n",
       "  0.28307024483680726,\n",
       "  0.28653190774917603,\n",
       "  0.31193826751708986,\n",
       "  0.30882324714660647],\n",
       " 'val_acc': [0.8773999994277955,\n",
       "  0.8867999994277954,\n",
       "  0.8833999998092651,\n",
       "  0.8729999999046326,\n",
       "  0.8793999992370606],\n",
       " 'loss': [0.5582705538431804,\n",
       "  0.254524738963445,\n",
       "  0.22519725665251414,\n",
       "  0.2038707525173823,\n",
       "  0.1875746687412262],\n",
       " 'acc': [0.8035333334287008,\n",
       "  0.9034666665077209,\n",
       "  0.9110666667620341,\n",
       "  0.9213333335240682,\n",
       "  0.9261333331108094]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction of the 34th in the predicted dataset is:  [1]\n",
      "The prediction of the 34th in the test dataset is:  1\n"
     ]
    }
   ],
   "source": [
    "predict_test = model.predict_classes(X_test[[34], :])\n",
    "print(\"The prediction of the 34th in the predicted dataset is: \", predict_test)\n",
    "print(\"The prediction of the 34th in the test dataset is: \", y_test[34])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the vocabulary of data\n",
    "# sentences = tokenizer.texts_to_matrix(sentences)\n",
    "\n",
    "max_review_len= max([len(s.split()) for s in sentences])\n",
    "vocab_size= len(tokenizer.word_index)+1\n",
    "sentences = tokenizer.texts_to_sequences(sentences)\n",
    "padded_docs= pad_sequences(sentences,maxlen=max_review_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,  33, 498,   8],\n",
       "       [  0,   0,   0, ..., 194, 413, 160],\n",
       "       [  0,   0,   0, ...,   8,   1, 188],\n",
       "       ...,\n",
       "       [  0,   0,   0, ..., 235, 170, 471],\n",
       "       [  0,   0,   0, ...,  40,  32,  69],\n",
       "       [  0,   0,   0, ...,   5,  97,  14]], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, y, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(2000, 100, input_length=max_review_len))\n",
    "model.add(Flatten())\n",
    "model.add(layers.Dense(300, activation='relu'))\n",
    "model.add(layers.Dense(20, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "15000/15000 [==============================] - 484s 32ms/step - loss: 6.0468 - acc: 0.6221 - val_loss: 5.9895 - val_acc: 0.6284\n",
      "Epoch 2/5\n",
      "15000/15000 [==============================] - 592s 39ms/step - loss: 6.0626 - acc: 0.6239 - val_loss: 5.9895 - val_acc: 0.6284\n",
      "Epoch 3/5\n",
      "15000/15000 [==============================] - 555s 37ms/step - loss: 6.0626 - acc: 0.6239 - val_loss: 5.9895 - val_acc: 0.6284\n",
      "Epoch 4/5\n",
      "15000/15000 [==============================] - 561s 37ms/step - loss: 6.0626 - acc: 0.6239 - val_loss: 5.9895 - val_acc: 0.6284\n",
      "Epoch 5/5\n",
      "15000/15000 [==============================] - 617s 41ms/step - loss: 6.0626 - acc: 0.6239 - val_loss: 5.9895 - val_acc: 0.6284\n",
      "5000/5000 [==============================] - 8s 2ms/step\n",
      "Evaluation result on Test Data : Loss = 5.989485542297364, accuracy = 0.6284\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,verbose=1,validation_data=(X_test,y_test),epochs=5)\n",
    "[test_loss, test_acc] = model.evaluate(X_test, y_test)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # Accuracy\n",
    "\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train =fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = newsgroups_train.data\n",
    "y = newsgroups_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing data\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "# getting the vocabulary of data\n",
    "# sentences = tokenizer.texts_to_matrix(sentences)\n",
    "\n",
    "max_review_len= max([len(s.split()) for s in sentences])\n",
    "vocab_size= len(tokenizer.word_index)+1\n",
    "sentences = tokenizer.texts_to_sequences(sentences)\n",
    "padded_docs= pad_sequences(sentences,maxlen=max_review_len)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, y, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8485 samples, validate on 2829 samples\n",
      "Epoch 1/5\n",
      "8485/8485 [==============================] - 3s 379us/step - loss: 15.2311 - acc: 0.0534 - val_loss: 15.2185 - val_acc: 0.0555\n",
      "Epoch 2/5\n",
      "8485/8485 [==============================] - 2s 286us/step - loss: 15.2289 - acc: 0.0542 - val_loss: 15.1524 - val_acc: 0.0597\n",
      "Epoch 3/5\n",
      "8485/8485 [==============================] - 2s 269us/step - loss: 15.2309 - acc: 0.0549 - val_loss: 15.2122 - val_acc: 0.0562\n",
      "Epoch 4/5\n",
      "8485/8485 [==============================] - 2s 276us/step - loss: 15.2455 - acc: 0.0541 - val_loss: 15.0926 - val_acc: 0.0636\n",
      "Epoch 5/5\n",
      "8485/8485 [==============================] - 2s 271us/step - loss: 15.1715 - acc: 0.0587 - val_loss: 15.0210 - val_acc: 0.0679\n",
      "2829/2829 [==============================] - 0s 92us/step\n",
      "Evaluation result on Test Data : Loss = 15.021003462224092, accuracy = 0.06786850478517246\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Dense(300,input_dim=11821, activation='relu'))\n",
    "model.add(layers.Dense(20, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "history=model.fit(X_train,y_train, epochs=5, verbose=True, validation_data=(X_test,y_test), batch_size=256)\n",
    "[test_loss, test_acc] = model.evaluate(X_test,y_test)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
